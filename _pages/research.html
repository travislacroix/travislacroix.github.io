---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<style> .indented { padding-left: 50pt; padding-right: 50pt; } </style>

<p>My research centres on philosophical questions surrounding the dynamics of complex social systems, often using formal tools from evolutionary game theory. This work includes interrelated projects in the areas of (1) ethically-aligned artificial intelligence, (2) social dynamics, norms, and conventions, and, more recently, (3) the philosophy of autism. Some current research projects are described below. (Click through for details.)</p>
   
<h3> 1. Philosophy and Ethics of AI and Emerging Technologies.</h3> 

<p>My main research programme surrounds the philosophy and ethics of AI. My primary focus has been on value-alignment problems in the context of AI systems. As these systems become more sophisticated and more deeply embedded in society, it will become increasingly essential to ensure that we can maintain control of them and that the decisions and actions they take are aligned with our values. My research emphasises that ensuring value alignment for AI systems requires more than just translating our best normative theories into a programming language.</p>

<details>
   <summary><a> <strong> Artificial Intelligence and the Value-Alignment Problem </strong> <br></a> </summary>
   <div class="indented">
      <p>
         The value-alignment problem for AI asks how we can ensure that the ‘values’ (objective functions) of artificial systems align with humanity’s values. One component of this problem is technical (how do we encode values or principles in AI systems?), and one component is normative (what values or principles are the ‘correct’ ones to encode in AI systems?). </p> <p>One of my main research projects is an interdisciplinary treatment of value-alignment problems for AI systems. Although an intuitive understanding of value alignment is superficially useful, it should be clear that it raises more questions than answers:
         <ul>
            <li> Are the ‘values of humanity’ individual values? Aggregate values? 
               <li> Whose values are they? How should they be determined? 
                  <li> How can a conception of value alignment deal with the variation of values across culture or time? 
      </ul>
      My approach to value alignment is differentiated from the extant literature in machine learning insofar as, in my analysis, value-alignment problems arise from the dynamics of multi-agent interactions.</p> <p> Thus, rather than focusing on which values are the right ones and how to implement them, I discuss the socio-dynamic contexts that give rise to value-alignment problems in the first place. This treatment provides a conceptual basis for understanding what value-alignment problems are (and how they are generated) in addition to shedding light on similar features between value-alignment problems for AI systems and other dynamic interactions (e.g., between human agents). </p> <p> One key consequence of this approach is that, outside of the intentional misuse of AI systems by bad-faith actors, every problem arising in machine ethics—bias and fairness; transparency, explainability, and opacity; control problems, etc.—can be cashed out in terms of value alignment. </p> <p> Understanding value-alignment problems in terms of their structural features underscores that we will not find solutions in the manipulation or re-configuration of training data or alternative algorithms.
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Language and Value Alignment </strong> <br></a> </summary>
   <div class="indented">
      <p>
         In recent and ongoing work, I argue that linguistic communication is necessary for robust value alignment. Therefore, understanding the fundamental principles involved in the (biological or cultural) evolution of effective communication may lead to innovative communication methods for AI systems. The importance of linguistic communication for coordinating social values and norms in complex social systems is often taken for granted in contemporary research on value alignment for AI. However, there are important evolutionary connections between social norms and linguistic communication. </p> <p> One goal of this research is to demonstrate the importance of understanding the biological origins of language as a path to achieving moral behaviour in AI systems.
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Normativity and Artificial Intelligence </strong> <br></a> </summary>
   <div class="indented">
      <p>
         Along with a multi-disciplinary research team I have been reviewing the literature on <i>artificial moral agency</i>. We seek to provide a systematic account of the most promising approaches to developing artificial moral agents, addressing normative pluralism, conflicts, and codification. A central research question outlines the key issues and most promising implementation approaches for high- and low-level agents. Additionally, we explore the main benchmarking approaches for assessing the level of functionality, accuracy, or moral competence of artificial moral agents. My recent single- and co-authored research in this area has sought to critically examine the very idea of benchmarking ethics for AI systems. I explore why current approaches in computer science fail in light of category mistakes involved in using moral dilemmas from philosophy as benchmarks for whether an AI system acts morally, in addition to discussing metaethical problems for benchmarking ethics for AI.
      </p>
   </div>
</details>

<h3>2. Social Dynamics, Norms, and Conventions.</h3>

<p>A second main focus of my research concerns questions surrounding social dynamics and the cultural and biological evolution of social phenomena. Primarily, this work has centred on language origins, but I have also studied the dynamics of other non-linguistic social phenomena. 
</p>

<details>
   <summary><a> <strong> Evolutionary Origins of Linguistic Communication </strong> <br></a> </summary>
   <div class="indented">
      <p>
         My work on the evolutionary origins of language stems from my dissertation. This work aims to address the following questions: 
         <ul>
            <li>What are the salient differences between the simple signalling systems that are ubiquitous in nature and the linguistic communication systems that are unique to humans? 
            </li>
            <li> Which of these salient features of natural language provides an empirically plausible target for explaining how linguistic communication systems may have evolved out of simpler systems of communication? 
            </li>
      </ul> 
      Many researchers think that if we could explain how some distinctive feature(s) of language evolved, we would have taken great strides in bridging the evolutionary gap between simple communication and natural language. The most common feature of natural language appealed to as a gap-bridging explanatory target is compositionality (and related features like hierarchy and recursion). I argue that the emphasis on compositional syntax in language-origins research is misguided. I examine the inherent asymmetry between the benefits of compositional syntax for senders and receivers in a signalling-game context. I further discuss the binary nature of compositionality, which precludes a gradualist explanation for how it evolved. Using comparative methods from evolutionary biology, I show that there is no empirical evidence for any relevant proto-compositional precursors in nature. This body of research suggests that it is a mistake to assume that since compositional syntax provides a crucial difference between language and simple communication, research on language origins must, therefore, centre on the evolution of compositional syntax itself.
      </p>
   <p>
      Instead, I propose that <i>reflexivity</i>—the ability to use language to talk about language—provides a plausible alternative explanatory target for language-origins research. Communication is a unique evolved mechanism to the extent that it can overtly influence the evolution of future communication. Once individuals learn to communicate, they may use those abilities to influence future communicative behaviour, leading to a positive feedback loop. I have demonstrated, via formal models, that reflexivity gives rise to rich compositional structures from which genuinely compositional syntax can emerge; but it emerges as a byproduct rather than an explicit target of evolutionary pressures. I further argue that reflexivity does not succumb to the problems that compositionality faces: role asymmetries are accounted for by the underlying mechanisms that give rise to reflexive communication systems; there exists empirical evidence of plausible precursors to reflexivity in nature; the precursors of reflexivity are genuinely graded. My research in this area provides initial evidence that reflexivity is a fruitful direction for explanations of language origins.
   </p>
   </div>
</details>

<details>
   <summary><a> <strong>Social Dynamics, Epistemology, and Justice </strong> <br></a> </summary>
   <div class="indented">
      <p>
        My work in social dynamics has also examined general (non-linguistic) social phenomena. This research stems primarily from work done under a grant from the National Science Foundation (USA) on Social Dynamics and Diversity in Epistemic Communities (PI: Cailin O’Connor, UC Irvine). We use formal models and simulation results to precisify arguments about various social phenomena. For example, my research on discrimination shows how small degrees of power give rise to radically inequitable distributions of resources between perceptibly distinct (but otherwise effectively identical) populations of individuals. This result has consequences for the viability of accounts of distributive justice. My research in social epistemology shows how false beliefs—concerning, for example, scientific information or ‘fake news’ items—spread and persist in a network of individuals, even when an explicit retraction is issued [18]. My future work in this area will provide novel models for analysing the social dynamics of epistemic accuracy, diversity, and inequity. This research programme is unified by the aim of understanding the role of social structure and interaction in epistemology and justice. With my co-author, Aydin Mohseni, I have also used stochastic models from game theory to analyse the cooperative challenge for AI Ethics guidelines 
      </p>
   </div>
</details>

<h3>3. The Philosophy of Autism and Autistic Philosophy.</h3> 

<p>Since Fall 2022, beginning with a graduate seminar that I designed to teach at Dalhousie University (Philosophy on the Spectrum: The Philosophy of Autism and Autistic Philosophy), I have been exploring philosophical research on autism spectrum disorders. Stuart Murray suggests that the very idea of an autistic person is a philosophical one. However, Kenneth Richman points out that ‘the philosophy of autism’ is not (yet) a subfield of philosophy insofar as philosophical work on autism has fallen primarily under ethics, philosophy of mind, philosophy of psychology, or philosophy of medicine.  Although the philosophy of autism is a fruitful research area within and without these narrower domains, my main research interest concerns (what I refer to as) autistic philosophy. </p>
   
<details>
   <summary><a> <strong>What Is Autistic Philosophy?</strong> <br></a> </summary>
   <div class="indented">
<p>Autistic philosophy starts from the realisation that much philosophical theorising has proceeded from a ‘neurotypical’ perspective, thus theoretically (and often in practice) grossly misunderstanding autism and perpetuating harmful stereotypes about autistics. The general approach to autistic philosophy is to take certain phenomena for granted in light of the existence and experience of autistic persons and then proceed to take these phenomena as fundamentally challenging existing philosophical theories from an autistic perspective. Thus, taking autistic perspectives as a starting point for one’s inquiry, it is possible to critically reflect on philosophical preconceptions, thereby challenging misconceptions about these notions based on neurodiversity within populations. As a philosopher of science, I am particularly interested in how autistic perspectives challenge dominant (neurotypical) scientific paradigms concerning epistemology, ontology, aetiology, and the very process of categorising, diagnosing, and pathologising autistic characteristics. </p>
   </div>
</details>

<h2>Books</h2>

<details>
   <summary>
      <a> 
         <strong> 
            Artificial Intelligence and the Value Alignment Problem: A Philosophical Introduction 
         </strong> 
      </a>
   </summary>
   <div class="indented">
      <p>
         <a href="https://broadviewpress.com/product/artificial-intelligence-and-the-value-alignment-problem/#tab-description"><strong>Available for purchase</strong></a>. <br> <br> <strong>From the publisher</strong>: "Written for an interdisciplinary audience, this book provides strikingly clear explanations of the many difficult technical and moral concepts central to discussions of ethics and AI. In particular, it serves as an introduction to the value alignment problem: that of ensuring that AI systems are aligned with the values of humanity. LaCroix redefines the problem as a structural one, showing students how many topics in AI ethics, from bias and fairness to transparency and opacity, can be understood as instances of the key problem of value alignment. Numerous case studies are presented throughout the book to highlight the significance of the issues at stake and to clarify the central role of the value alignment problem in the many ethical challenges facing the development and implementation of AI." <br>
      </p>
      <p>
         <b>Advanced Praise</b>: <br> <br>
         "Travis LaCroix’s book on value alignment is, without a doubt, the best I have read on AI ethics. I highly recommend it to anyone interested in the ethics of artificial intelligence. The text is intellectually rigorous, and many of its ideas are genuinely novel. I found his discussion of measuring value alignment particularly insightful, along with the appendix on superintelligence and the control problem, which provides valuable depth to the topic.” -- Martin Peterson, Texas A&M University" <br> <br>

"LaCroix's <i>Artificial Intelligence and the Value Alignment Problem</i> offers an insightful overview and evaluation of the predicament we find ourselves in with respect to machine learning. The book doesn't shy away from engaging with the mathematical background of these challenges, but it does so in a way that's intelligible to readers with limited mathematical experience. The structural characterization of the alignment problem(s) provides a great conceptual tool for exploring the ways that values are (or fail to be) incorporated in machine learning systems. The discussions of values are also inclusive, incorporating views from Western, Eastern, and Indigenous philosophy. This book offers an up-to-date introduction to the topic at a level suitable for undergraduates while also providing a novel analytic tool for anyone already working in the area of AI ethics." -- Gillman Payette, University of Calgary
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2025. <i>Artificial Intelligence and the Value Alignment Problem: A Philosophical Introduction</i>. Broadview Press.<br>
      </p>
   </div>
</details>

<h2>Peer-Reviewed Articles</h2>

<details>
   <summary><a> <strong> Autism and the Pseudoscience of Mind </strong> <br> <p style="text-indent: 20pt"><i>Psychological Inquiry</i> (Accepted)</p></a> </summary>
   <div class="indented">
      <p>
         The theory-of-mind-deficit explanation of autism proposes that autistics lack a theory of mind, that autism comprises a theory-of-mind deficit (strong version); or, that autistics often have difficulty with theory-of-mind abilities (weak version). A growing body of critical research demonstrates how these explanations of autistic behaviour fail—both empirically and theoretically. The strong version lacks explanatory adequacy, while the weak version is undermined by methodological and empirical flaws in theory-of-mind research. Together, these issues suggest that the "science" of theory of mind in the context of autism is, at best, bad science. Nonetheless, researchers continue to pursue this line of inquiry in autism studies—often moving the goalposts or offering ad hoc rationalisations to preserve the theoretical framework. This article critically examines the theory-of-mind-deficit explanation of autism, focusing particularly on the widely-held view that autistics exhibit difficulties with theory of mind—i.e., the weak version of the theory-of-mind-deficit explanation. Drawing from the philosophy of science, I argue that ongoing adherence to this view exhibits all the hallmarks of a degenerating research programme. Hence, the fact that scientists have not abandoned this hypothesis entails that the research programme is pseudoscientific.
      </p>
      <p>
          <a href="" title="Autism and the Pseudoscience of Mind, Psych Inq">[Official version forthcoming.]</a> <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. Accepted. "Autism and the Pseudoscience of Mind" <i>Psychological Inquiry</i>. 1-66.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> What Do Philosophers Talk About When They Talk About Autism? </strong> (with Alexis Amero and Benjamin Sidloski) <br> <p style="text-indent: 20pt"><i>Synthese</i> (Accepted)</p></a> </summary>
   <div class="indented">
      <p>
         Several anecdotal claims about the relationship between philosophical discourse and the subject of autism have been forwarded in recent years. This paper seeks to verify or debunk these descriptive claims by carefully examining the philosophical literature on autism. We conduct a comprehensive scoping review to answer the question, <i>what do philosophers talk about when they talk about autism?</i> This empirical work confirms that the philosophy of autism is underdeveloped as a subfield of philosophy. Moreover, the way that philosophers engage with autism is often unreflective and uncritical. As a result, much work in the discipline serves to perpetuate pathologising, dehumanising, and stigmatising misinformation about autistics and autistic behaviour. By highlighting the significant gaps in the philosophical literature on autism, this review aims to deepen our understanding of philosophical thought surrounding autism and contributes to ongoing dialogues pertaining to neurodiversity, madness, and disability rights more generally.
      </p>
      <p>
          <a href="https://doi.org/10.1007/s11229-025-05116-1" title="WDPTAWTTAA?, Synthese">[Official version available here]</a> (Open Access) <br>
          <a href="https://autphi.github.io/downloads/LaCroix-et-al-2025-WDPTAWTTAA-Synthese-Preprint.pdf" title="WDPTAWTTAA?, Preprint">[Pre-print available here.]</a> <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis, Alexis Amero, and Benjamin Sidloski. 2025. "What Do Philosophers Talk About When They Talk About Autism?" <i>Synthese</i>. 206(63):  1-47.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Information and Meaning in the Evolution of Compositional Signals </strong><br> <p style="text-indent: 20pt"><i>Journal of Logic, Language and Information</i> (2025)</p></a> </summary>
   <div class="indented">
      <p>
      This paper provides a formal treatment of the argument that syntax alone cannot give rise to compositionality in a signalling game context. This conclusion follows from the standard information-theoretic machinery used in the signalling game literature to describe the informational <i>content</i> of signals.  
      </p>
      <p>
         <a href="https://doi.org/10.1007/s10849-025-09433-z" title="Information and Meaning in the Evolution of Compositional Signals, Official">[Official version available here.]</a> (Open Access)<br>
         <a href="http://philsci-archive.pitt.edu/20716/" title="Information and Meaning in the Evolution of Compositional Signals, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite published version, if available.)<br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2025. "Information and Meaning in the Evolution of Compositional Signals." <i>Journal of Logic, Language and Information</i>. 34: 219-239.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Metaethical Perspectives on 'Benchmarking' AI Ethics </strong> (with Alexandra Sasha Luccioni) <br> <p style="text-indent: 20pt"><i>AI and Ethics</i> (2024)</p></a> </summary>
   <div class="indented">
      <p>
         Benchmarks are seen as the cornerstone for measuring technical progress in artificial intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to emotion recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is 'ethical'. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about 'values' (and 'value alignment') rather than 'ethics' when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial AI.
      </p>
      <p>
          <a href="https://doi.org/10.1007/s43681-025-00703-x" title="Metaethical Perspectives on Benchmarking, AI Ethics">[Official version available Here (Open Access).]</a> <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis, and Alexandra Sasha Luccioni. 2025 "Metaethical Perspectives on 'Benchmarking' AI Ethics." <i>AI and Ethics</i>. 5: 4029-4047.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> The Linguistic Dead Zone of Value-aligned Agency, Natural and Artificial </strong> <br> <p style="text-indent: 20pt"><i>Philosophical Studies</i> (2024)</p></a> </summary>
   <div class="indented">
      <p>
         The value alignment problem for artificial intelligence (AI) asks how we can ensure that the “values”—i.e., objective functions—of artificial systems are aligned with the values of humanity. In this paper, I argue that linguistic communication is a necessary condition for robust value alignment. I discuss the consequences that the truth of this claim would have for research programmes that attempt to ensure value alignment for AI systems—or, more loftily, those programmes that seek to design robustly beneficial or ethical artificial agents.
      </p>
      <p>
          <a href="https://doi.org/10.1007/s11098-024-02257-w" title="Linguistic Dead Zone, Phil. Studies">[Official version available Here (Open Access).]</a> <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2024. "The Linguistic Dead Zone of Value-Aligned Agency, Natural and Artificial." <i>Philosophical Studies</i>. 1-23.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Power by Association </strong> (with Cailin O'Connor) <br> <p style="text-indent: 20pt"><i>Ergo, an Open Access Journal of Philosophy</i> (2022)</p></a> </summary>
   <div class="indented">
      <p>
         We use tools from evolutionary game theory to examine how power might influence the cultural evolution of inequitable norms between discernible groups (such as gender or racial groups) in a population of otherwise identical individuals. Similar extant models always assume that power is homogeneous across a social group. As such, these models fail to capture situations where individuals who are not themselves disempowered nonetheless end up disadvantaged in bargaining scenarios by dint of their social group membership. Thus, we assume that there is heterogeneity in the groups in that some individuals are more powerful than others.
      </p>
      <p>
         Our model shows that even when most individuals in two discernible sub-groups are relevantly identical, powerful individuals can affect the social outcomes for their entire group; this results in power by association for their in-group and a bargaining disadvantage for their out-group. In addition, we observe scenarios like those described where individuals who are more powerful will get less in a bargaining scenario because a convention has emerged disadvantaging their social group.
      </p>
      <p>
          <a href="https://journals.publishing.umich.edu/ergo/article/id/2230/" title="Power by Association, Ergo">[Official version available here (Open Access).]</a> <br>
         <a href="http://philsci-archive.pitt.edu/14318/" title="Power by Association, Preprint">[PhilSci-Archive Preprint available here.]</a> (Please cite published version, if available.)<br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis and Cailin O'Connor. 2022. "Power by Association." <i>Ergo</i>. 8(29): 163-189.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Est-ce que vous Compute? Code-Switching, Culutral Identity, and AI </strong> (with Arianna Falbo) <br> <p style="text-indent: 20pt"><i>Feminist Philosophical Quarterly</i> (2022)</p></a> </summary>
   <div class="indented">
      <p>
         Cultural code-switching concerns how we adjust our overall behaviours, manners of speaking, and appearance in response to a perceived change in our social environment. We defend the need to investigate cultural code-switching capacities in artificial intelligence systems. We explore a series of ethical and epistemic issues that arise when bringing cultural code-switching to bear on artificial intelligence. Building upon Dotson's (2014) analysis of testimonial smothering, we discuss how emerging technologies in AI can give rise to epistemic oppression, and specifically, a form of self-silencing that we call 'cultural smothering'. By leaving the socio-dynamic features of cultural code-switching unaddressed, AI systems risk negatively impacting already-marginalised social groups by widening opportunity gaps and further entrenching social inequalities.
      </p>
      <p>
         <a href="https://ojs.lib.uwo.ca/index.php/fpq/article/view/14264" title="Est-ce que vous compute, FPQ">[Official version available here (Open Access).]</a> <br>
         <a href="https://arxiv.org/abs/2112.08256" title="Est-ce que vous compute, Preprint">[arXiv preprint available here.]</a> (Please cite published version, if available.)<br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> Falbo, Arianna and Travis LaCroix. 2022. "Est-ce que vous compute? Code-switching, cultural identity and AI." <i>Feminist Philosophical Quarterly</i>. 8(3-4): 9:1-9:24.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> The Tragedy of the AI Commons </strong> (with Aydin Mohseni) <br> <p style="text-indent: 20pt"><i>Synthese</i> (2022)</p></a> </summary>
   <div class="indented">
      <p>
         Policy and guideline proposals for ethical artificial-intelligence research have proliferated in recent years. These are supposed to guide the socially-responsible development of AI for the common good. However, there typically exist incentives for non-cooperation (i.e., non-adherence to such policies and guidelines); and, these proposals often lack effective mechanisms to enforce their own normative claims. The situation just described constitutes a social dilemma—namely, a situation where no one has an individual incentive to cooperate, though mutual cooperation would lead to the best outcome for all involved. In this paper, we use stochastic evolutionary game dynamics to model this social dilemma in the context of the ethical development of artificial intelligence. This formalism allows us to isolate variables that may be intervened upon, thus providing actionable suggestions for increased cooperation amongst numerous stakeholders in AI. Our results show how stochastic effects can help make cooperation viable in such a scenario. They suggest that coordination for a common good should be attempted in smaller groups in which the cost for cooperation is low, and the perceived risk of failure is high. This provides insight into the conditions under which we should expect such ethics proposals to be successful with regard to their scope, scale, and content.
 <br>
      </p>
      <p>
         <a href="https://rdcu.be/cW9XV" title="Tragedy of the AI Commons, Synthese">[Official version available here.]</a> <br>
         <a href="https://arxiv.org/abs/2006.05203" title="Tragedy of the AI Commons, Draft">[arXiv preprint available here.]</a> (Please cite official version, if available.) <br>
         <a href="https://player.vimeo.com/video/505335066?h=4403910eed" title="Recorded Talk, Vimeo">[Recorded talk available here.]</a> <br>
         <a href="https://travislacroix.github.io/files/LaCroix-Mohseni-PSA-Poster-Final-36x48.pdf" title="Tragedy of the AI Commons, Poster">[Poster available here.]</a>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis and Aydin Mohseni. 2022. "The Tragedy of the AI Commons." <i>Synthese</i> 200: 289:1-289:33. <a href=https://doi.org/10.1007/s11229-022-03763-2>https://doi.org/10.1007/s11229-022-03763-2</a><br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Moral Dilemmas for Moral Machines </strong> <br> <p style="text-indent: 20pt"><i>AI and Ethics</i> (2022)</p></a> </summary>
   <div class="indented">
      <p>
         Autonomous systems are being developed and deployed in situations that may require some degree of ethical decision-making ability. As a result, research in machine ethics has proliferated in recent years. This work has included using moral dilemmas as validation mechanisms for implementing decision-making algorithms in ethically-loaded situations. Using trolley-style problems in the context of autonomous vehicles as a case study, I argue (1) that this is a misapplication of philosophical thought experiments because (2) it fails to appreciate the purpose of moral dilemmas, and (3) this has potentially catastrophic consequences; however, (4) there are uses of moral dilemmas in machine ethics that are appropriate and the novel situations that arise in a machine-learning context can shed some light on philosophical work in ethics.
      </p>
      <p>
         <a href="https://rdcu.be/cIMmt" title="Moral dilemmas for moral machines, AI Ethics">[Official version available here.]</a> <br>

         <a href="http://philsci-archive.pitt.edu/20339/" title="Moral dilemmas for moral machines, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite published version, if available.)<br>
         
         <a href="https://arxiv.org/abs/2203.06152" title="Moral dilemmas for moral machines, Preprint">[arXiv Preprint available here.]</a> (Please cite published version, if available.)<br>

      </p>
      Summaries of this research have been published on the <a href"https://blog.apaonline.org/2022/06/16/a-category-mistake-benchmarking-ethical-decisions-for-ai-systems-using-moral-dilemmas%EF%BF%BC/" title="Moral Dilemmas for Moral Machines, APA Blog">Blog of the American Philosophical Association</a> and the <a href="https://montrealethics.ai/moral-dilemmas-for-moral-machines/" title="Moral Dilemmas for Moral Machines, AI Ethics Brief">AI Ethics Brief</a>, distributed by the Montreal AI Ethics Institute.
      <p>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2022. "Moral dilemmas for moral machines." <i>AI and Ethics</i>. 2: 737-746. <a href="https://doi.org/10.1007/s43681-022-00134-y">[https://doi.org/10.1007/s43681-022-00134-y]</a>.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Using Logic to Evolve More Logic: Composing Logical Operators via Self-Assembly </strong> <br> <p style="text-indent: 20pt"><i>British Journal for the Philosophy of Science</i> (2022)</p></a> </summary>

   </summary>
   <div class="indented">
      <p>
         I consider how complex logical operations might self-assemble in a signalling-game context via composition of simpler underlying dispositions. On the one hand, agents may take advantage of pre-evolved dispositions; on the other hand, they may co-evolve dispositions as they simultaneously learn to combine them to display more complex behaviour. In either case, the evolution of complex logical operations can be more efficient that evolving such capacities from scratch. Showing how complex phenomena like these might evolve provides an additional path to the possibility of evolving more or less rich notions of compositionality. This helps provide another facet of the evolutionary story of how sufficiently rich, human-level cognitive or linguistic capacities may arise from simpler precursors.
         <br> </p>
         <p>
         <a href="https://doi.org/10.1093/bjps/axz049" title="Using Logic to Evolve More Logic, BJPS">[Official version available here.]</a> <br>
         <a href="http://philsci-archive.pitt.edu/16658/" title="Using Logic to Evolve More Logic, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite official version.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "Using Logic to Evolve More Logic: Composing Logical Operators via Self-Assembly." <i>British Journal for the Philosophy of Science</i> (2022) 73(2): 407-437. <br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Epistemology and the Structure of Language </strong> (with Jeffrey A. Barrett) <br> <p style="text-indent: 20pt"><i>Erkenntnis</i> (2022)</p></a> </summary>

   </summary>
   <div class="indented">
      <p>
         We are concerned here with how structural properties of language may evolve to reflect features of the world in which it evolves. As a concrete example, we will consider how a simple term language might evolve to support the principle of indifference over state descriptions in that language. The point is not that one is justified in applying the principle of indifference to state descriptions in natural language. Rather, it is that one should expect a language that has evolved in the context of facilitating successful action to reflect probabilistic features of the world in which it evolved.
      <br> </p>
         <p>
         <a href="https://rdcu.be/cW9Y8" title="Epistemology and the Structure of Language, Erkenntnis">[Official version available here.]</a> <br>
         <a href="http://philsci-archive.pitt.edu/16986/" title="Epistemology and the Structure of Language, Preprint">[PhilSci-Archive Preprint available here.]</a> (Please cite official version.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> Barrett, Jeffrey A. and Travis LaCroix. 2020. "Epistemology and the Structure of Language." <i>Erkenntnis</i> (2022) 87: 953-967. https://doi.org/10.1007/s10670-020-00225-4<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong>Reflexivity, Functional Reference, and Modularity: Alternative Targets for Language Origins</strong> <br> <p style="text-indent: 20pt"><i>Philosophy of Science</i> (2021)</p></a> </summary>
   <div class="indented">
      <p>
         Researchers in language origins typically try to explain how compositional communication might evolve to bridge the gap between animal communication and natural language. However, as an explanatory target, compositionality has been shown to be problematic for a gradualist approach to the evolution of language.  In this paper, I suggest that <i>reflexivity</i> provides an apt and plausible alternative target which does not succumb to the problems that compositionality faces. I further explain how <i>proto</i>-reflexivity, which depends upon functional reference, gives rise to complex communication systems via modular composition.
      <p>
         <a href="https://doi.org/10.1086/715217" title="Reflexivity, Functional Reference, and Modularity, PhoS">[Official version available here.]</a><br>

         <a href="http://philsci-archive.pitt.edu/19797/" title="Reflexivity, Functional Reference, and Modularity, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite official version, if available.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2020. "Reflexivity, Functional Reference, and Modularity: Alternative Targets for Language Origins." <i>Philosophy of Science</i> (2021) 8(5): 1234-1245.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> The Dynamics of Retraction in Epistemic Networks </strong> (with Cailin O'Connor and Anders Geil) <br> <p style="text-indent: 20pt"><i>Philosophy of Science</i> (2021)</p></a> </summary>
   <div class="indented">
      <p>
         Sometimes retracted scientific information is used and propagated long after it is understood to be misleading.  Likewise, sometimes retracted news items spread and persist, even after it has been publicly established that they are false.  In this paper, we use agent-based models of epistemic networks to explore the dynamics of retraction.  In particular we focus on why false beliefs might persist, even in the face of retraction.
      </p>
      <p>
         <a href="https://doi.org/10.1086/712817" title="Dynamics of Retraction, PhoS">[Official version available here.]</a><br>
         <a href="http://philsci-archive.pitt.edu/17088/" title="Dynamics of Retraction, Draft">[PhilSci-Archive preprint available here.]</a> (Please cite published version, if available.)<br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis, Anders Geil, and Cailin O'Connor. 2020. "The Dynamics of Retraction in Epistemic Networks." <i>Philosophy of Science</i> (2021) 88(3): 415-438. https://doi.org/10.1086/712817<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Communicative Bottlenecks Lead to Maximal Information Transfer </strong> <br> <p style="text-indent: 20pt"><i>Journal of Experimental and Theoretical Artificial Intelligence</i> (2020)</p></a> </summary>

   </summary>
   <div class="indented">
      <p>
         This paper presents new analytic and numerical analysis of signalling games that give rise to informational bottlenecks—that is to say, signalling games with more state/act pairs than available signals to communicate information about the world. I show via simulation that agents learning to coordinate tend to favour partitions of nature which provide maximal information transfer. This is true in spite of the fact that nothing from an initial analysis of the stability properties of the underlying signalling game suggests that this should be the case. As a first pass to explain this, I note that the underlying structure of our model favours maximal information transfer in regard to the simple combinatorial properties of the ways in which the agents might partition nature into kinds. However, I suggest that this does not perfectly capture the empirical results; thus, several open questions remain.
      </p>
      <p>
         <a href="http://dx.doi.org/10.1080/0952813X.2020.1716857" title="Communicative Bottlenecks, JETAI">[Official version available here.]</a>
      <br>
         <a href="http://philsci-archive.pitt.edu/16843/" title="Communicative Bottlenecks, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite official version.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br>LaCroix, Travis. 2020. "Communicative Bottlenecks Lead to Maximal Information Transfer." <i>Journal of Experimental and Theoretical Artificial Intelligence</i> (2020) 32(6): 997-1014.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Evolutionary Explanations of Simple Communication: Signalling Games &amp; Their Models </strong> <br> <p style="text-indent: 20pt"><i>Journal for General Philosophy of Science / Zeitschrift f&uuml;r allgemeine Wissenschaftstheorie</i> (2020)</p></a> </summary>
   <div class="indented">
      <p>         
         This paper applies the theoretical criteria laid out by D'arms et al. (1998) to various aspects of evolutionary models of signalling. The question that D'Arms et al. seek to answer can be formulated as follows: Are the models that we use to explain the phenomena in question conceptually adequate? The conceptual adequacy question relates the formal aspects of the model to those aspects of the natural world that the model is supposed to capture. Moreover, this paper extends the analysis of D'Arms et al. by asking the following additional question: Are the models that we use sufficient to explain the phenomena in question? The sufficiency question ask what formal resources are minimally required in order for the model to get the right results most of the time.
      </p>
      <p>
         <a href="https://rdcu.be/cW9YS" title="Evolutionary Explanations of Simple Communication, JGPS">[Official version available here.]</a><br>
         <a href="http://philsci-archive.pitt.edu/16604/" title="Evolutionary Explanations of Simple Communication, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite official version.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "Evolutionary Explanations of Simple Communication: Signalling Games &amp; Their Models." <i>Journal for General Philosophy of Science / Zeitschrift f&uuml;r allgemeine Wissenschaftstheorie</i> (2020) 51(1): 19-43.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> On Salience and Signalling in Sender-Receiver Games: Partial-Pooling, Learning, and Focal Points </strong> <br> <p style="text-indent: 20pt"><i>Synthese</i> (2020)</p></a> </summary>
   <div class="indented">
      <p>
         I introduce an extension of the Lewis-Skyrms signaling game, analysed from a dynamical perspective via simple reinforcement learning. In Lewis' (Convention, Blackwell, Oxford, 1969) conception of a signaling game, salience is offered as an explanation for how individuals may come to agree upon a linguistic convention. Skyrms (Signals: evolution, learning & information, Oxford University Press, Oxford, 2010a) offers a dynamic explanation of how signaling conventions might arise presupposing no salience whatsoever. The extension of the atomic signaling game examined here—which I refer to as a <i>salience game</i>—introduces a variable parameter into the atomic signaling game which allows for degrees of salience, thus filling in the continuum between Skyrms' and Lewis' models. The model does not presuppose any salience at the outset, but illustrates a process by which accidentally evolved salience is amplified, to the benefit of the players. It is shown that increasing degrees of salience allow populations to avoid sub-optimal pooling equilibria and to coordinate upon conventions more quickly. <br>
      </p>
      <p>
         <a href="https://rdcu.be/cW9YR" title="Salience and Signaling, Synthese">[Official version available here.]</a><br>
         <a href="http://philsci-archive.pitt.edu/16270/" title="Salience and Signaling, Preprint">[PhilSci-Archive preprint available here.]</a> (Please cite official version.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2018. "On Salience and Signaling in Sender-Receiver Games: Partial Pooling, Learning, and Focal Points." <i>Synthese</i> (2020) 197(4): 1725-1747. <br>
      </p>
      </div>
</details>

<h2>Book Chapters</h2>

<details>
   <summary><a> <strong> Ethics and Deep Learning </strong> (with Simon J. D. Prince) <br> <p style="text-indent: 20pt">Chapter 21 in Simon J. D. Prince (2023) <i>Understanding Deep Learning</i></p></a> </summary>
   <div class="indented">
      <p>
         An overview of some ethical issues arising from deep learning approaches to artificial intelligence research.
      </p>
            <p>
         <a href="https://udlbook.github.io/udlbook/" title="UDL Book">[Complete book draft available here.]</a><br>               
      </p>
            <p>
         <b>Recommended citation</b>: <br>LaCroix, Travis, and Simon J. D. Prince. 2023. "Ethics and Deep Learning". Chapter 21 in Simon J. D. Prince, <i>Understanding Deep Learning</i>. Cambridge, MA: The MIT Press. 420-435.<br>
      </p>
   </div>
</details>

<h2>Conference Proceedings</h2>

<details>
   <summary><a> <strong> Emergent Communication under Competition </strong> (with Michael Noukhovitch, Angeliki Lazaridou, and Aaron Courville) <br> <p style="text-indent: 20pt"><i>Autonomous Agents and Multiagent Systems (AAMAS 2021)</i></p></a> </summary>
   <div class="indented">
      <p>
         Current literature in machine learning has only negative results for learning to communicate between competitive agents using vanilla RL. We introduce a modified sender-receiver game to study the spectrum of partially-competitive scenarios and show communication can indeed emerge in this setting. We empirically demonstrate three key takeaways for future research. First, we show that communication is proportional to cooperation, and it can occur for partially competitive scenarios using standard learning algorithms. Second, we highlight the difference between communication and manipulation and extend previous metrics of communication to the competitive case. Third, we investigate the negotiation game where previous work failed to learn communication between independent agents. We show that, in this setting, both agents must benefit from communication for it to emerge. Finally, with a slight modification to the game, we successfully learn to communicate between competitive agents. We hope this work overturns misconceptions and inspires more research in competitive emergent communication.
      </p>
            <p>
         <a href="https://dl.acm.org/doi/10.5555/3463952.3464066" title="Emergent Communication, AAMAS">[Official version available here.]</a><br>

         <a href="https://arxiv.org/abs/2101.10276" title="Emergent Communication, Preprint">[arXiv preprint available here.]</a> (Please cite published version, if available.)<br>
               
         <a href="https://slideslive.com/38954927/emergent-communication-under-competition" title="Recorded Talk, SlidesLive">[Recorded Talk (Noukhovitch) Available Here.]</a> <br>
      </p>
            <p>
         <b>Recommended citation</b>: <br>Noukhovitch, Michael, Travis LaCroix, Angeliki Lazaridou, and Aaron Courville. 2021. Emergent Communication under Competition. In U. Endriss, A. Nowé, F. Dignum, and A. Lomuscio (eds.), <i>Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)</i>, London, UK, 3-7 May 2021, International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS) 974-982.<br>
      </p>
   </div>
</details>

<details>
   <summary><a> <strong> Biology and Compositionality: Empirical Considerations for Emergent-Communication Protocols </strong> <br> <p style="text-indent: 20pt"><i>NeurIPS 2019 workshop Emergent Communication: Towards Natural Language</i> (2019)</p></a> </summary>

   </summary>
   <div class="indented">
      <p>
         Significant advances have been made in artificial systems by using biological systems as a guide. However, there is often little interaction between computational models for emergent communication and biological models of the emergence of language. Many researchers in language origins and emergent communication take compositionality as their primary target for explaining how simple communication systems can become more like natural language. However, there is reason to think that compositionality is the wrong target on the biological side, and so too the wrong target on the machine-learning side. As such, the purpose of this paper is to explore this claim. This has theoretical implications for language origins research more generally, but the focus here will be the implications for research on emergent communication in computer science and machine learning—specifically regarding the types of programmes that might be expected to work and those which will not. I further suggest an alternative approach for future research which focuses on reflexivity, rather than compositionality, as a target for explaining how simple communication systems may become more like natural language. I end by providing some reference to the language origins literature that may be of some use to researchers in machine learning.
         <br> </p>
         <p>
         <a href="https://arxiv.org/abs/1911.11668" title="Biology and Compositionality, ArXiv">[ArXiv preprint available here.]</a> <br>
         <a href="http://travislacroix.github.io/files/LaCroix-NeurIPS-Poster-33x46-Biology-and-Compositionality.pdf" title="Biology and Compositionality, Poster">[Poster available here.]</a> <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "Biology and Compositionality: Empirical Considerations for Emergent-Communication Protocols." <i>arXiv preprint</i>: 1911.11668. https://arxiv.org/abs/1911.11668. <br>
      </p>
   </div>
</details>

<h2>Book Reviews</h2>

<details>
   <summary><a> <strong> Review of Ronald J. Planer and Kim Sterelny’s <i>From Signal to Symbol</i> </strong> <br> <p style="text-indent: 20pt"><i>Philosophy of Science</i> (2023)</p></a> </summary>

   </summary>
   <div class="indented">
      <p>
         Review of Ronald J. Planer and Kim Sterelny’s From Signal to Symbol – Ronald J. Planer and Kim Sterelny, <i>From Signal to Symbol: The Evolution of Language</i>. Cambridge: The MIT Press (2021), 296 pp., $35.00 (hardcover)
         <br> </p>
         <p>
         <a href="https://doi.org/10.1017/psa.2023.75" title="P&S Review, PhoS">[Official version available here.]</a> 
         </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2023. "Review of Ronald J. Planer and Kim Sterelny's <i>From Signal to Symbol</i>." <i>Philosophy of Sceince</i> (Forthcoming): 1-4. <a href="https://doi.org/10.1017/psa.2023.75"> https://doi.org/10.1017/psa.2023.75 </a>. <br>
      </p>
   </div>
</details>

<h2>Dissertation</h2>

<details>
   <summary><a> <strong> Complex Signals: Reflexivity, Hierarchical Structure, and Modular Composition </strong></a></summary>
   <div class="indented">
      <p>
         My dissertation argues that what drives the emergence of complex communication systems is a process of modular composition, whereby independent communicative dispositions combine to create more complex dispositions. This challenges the dominant view on the evolution of language, which attempts to resolve the explanatory gap between communication and language by demonstrating how complex syntax evolved. My research shows that these accounts fail to maintain sensitivity to empirical data: genuinely compositional syntax is extremely rare or non-existent in nature. In contrast, my research prioritises the reflexivity of natural language—the ability to use language to talk about language—as an alternative explanatory target. 
      </p>
      <p>
         The first part of my dissertation provides the philosophical foundation of this novel account using the theoretical framework of Lewis-Skyrms signalling games and drawing upon relevant work in evolutionary biology, linguistics, cognitive systems, and machine learning. Chapter 1 introduces the signalling game and contextualises it with respect to problems in the realm of traditional philosophy of language. Chapter 2 examines empirical data from biology and linguistics and argues that complex syntax is not the most apt explanatory target for how language might have evolved out of simple communication. Chapter 3 then argues that the reflexivity of language is a more fruitful property to consider, showing how reflexivity aids the evolution of complex communication via a process of modular composition. This connects parallel research in the evolution of language, cognitive systems, and machine learning paradigms. Once such complexity is exhibited, at a small scale, it may lead to a 'feedback loop' between communication and cognition that gives rise to the complexity we see in natural language.
      </p>
         The second part of my dissertation provides a set of models, along with analytic and simulation results, that show precisely how (and under what circumstances) this process of modular composition is supposed to work.
      </p>
   <p>
         A more detailed summary of this work can be read <a href="https://travislacroix.github.io/files/ND-Dissertation-Summary.pdf" title="Dissertation Summary"> HERE.</a>
   </p>
   <p>
         <a href="https://escholarship.org/uc/item/5328x080" title="Complex Signals">[Official version available here.]</a><br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2020. Complex Signals: Reflexivity, Hierarchical Structure, and Modular Composition. <i>UC Irvine</i>. ProQuest ID: LaCroix_uci_0030D_16213. Merritt ID: ark:/13030/m5ps345j. Retrieved from https://escholarship.org/uc/item/5328x080<br>
      </p>
   </div>
</details>

<h2>Under Review</h2>

<details>
   <summary><a> <strong> Accounting for Polysemy and Role Asymmetry in the Evolution of Compositional Signals </strong></a> </summary>
   <div class="indented">
      <p>
         Several formal models of signalling conventions have been proposed to explain how and under what circumstances compositional signalling might evolve. I suggest that these models fail to give a plausible account of the evolution of compositionality because (1) they apparently take <i>linguistic</i> compositionality as their target phenomenon, and (2) they are insensitive to role asymmetries inherent to the signalling game. I further suggest that, rather than asking how signals might come to be compositional, we must clarify what it would mean for signals to <i>be</i> compositional to begin with.
 <br>
      </p>
      <p>
         <a href="https://travislacroix.github.io/files/ND-Polysemy-and-Role-Asymmetry.pdf" title="Polysemy and Role Asymmetry, Draft">[Unpublished draft available here.]</a> (Please cite official version, if available.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "Accounting for Polysemy and Role-Asymmetry in the Evolution of Compositional Signals." <i>Unpublished Manuscript</i>. May 2019, PDF File.<br>
      </p>
   </div>
</details>

<br>

<details>
   <summary><a> <strong> The Correction Game or, How Pre-Evolved Communicative Dispositions Might Affect Communicative Dispositions </strong></a> </summary>
   <div class="indented">
      <p>
         How might pre-evolved communicative dispositions affect how individuals learn to communicate in a novel context? I present a model of learning that varies the reward for coordination in the signalling game framework under simple reinforcement learning as a function of the agents' actions. The model takes advantage of a type of modular compositional communicative bootstrapping by which the sender and receiver use pre-evolved communicative dispositions—a "yes/no" command—to evolve new dispositions.
 <br>
      </p>
      <p>
         <a href="https://travislacroix.github.io/files/ND-Correction-Game.pdf" title="Correction Game, Draft">[Unpublished draft available here.]</a> (Please cite official version, if available.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "The Correction Game or, How Pre-Evolved Communicative Dispositions Might Affect Communicative Dispositions." <i>Unpublished Manuscript</i>. April 2019, PDF File.<br>
      </p>
   </div>
</details>

<br>

<details>
   <summary><a> <strong> Learning From Learning Machines: Optimisation, Rules, and Social Norms </strong> (with Yoshua Bengio) </a> </summary>
   <div class="indented">
      <p>
         There is an analogy between machine learning systems and economic entities in that they are both adaptive, and their behaviour is specified in a more or less explicit way. It appears that the area of AI that is most analogous to the behaviour of economic entities is that of <i>morally good decision-making</i>, but it is an open question as to how precisely moral behaviour can be achieved in an AI system. This paper explores the analogy between these two complex systems, and we suggest that a clearer understanding of this apparent analogy may help us forward in both the socio-economic domain and the AI domain: known results in economics may help inform feasible solutions in AI safety, but also known results in AI may inform economic policy. If this claim is correct, then the recent successes of deep learning for AI suggest that more <i>implicit</i> specifications work better than explicit ones for solving such problems. 
      </p>
      <p>
         <a href="https://arxiv.org/abs/2001.00006" title="Learning from Learning Machines, Draft">[arXiv Preprint available here.]</a> (Please cite published version, if available.)<br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis and Yoshua Bengio. 2019. "Learning from Learning Machines: Optimisation, Rules, and Social Norms." <i>arxiv.org/abs/2001.00006</i>.<br>
      </p>
   </div>
</details>

<br>

<details>
   <summary><a> <strong> Relative Principals, Pluralistic Alignment, and the Structural Value Alignment Problem </strong></a> </summary>
   <div class="indented">
      <p>
         The value alignment problem for artificial intelligence (AI)—ensuring AI systems act in accordance with human values—is framed as a purely technical or normative challenge, often focused on hypothetical future AI systems. This paper presents a reconceptualisation of value alignment as a structural problem grounded in real-world human–AI interactions. Drawing on the principal–agent framework from economics, value misalignment is described as arising whenever (a) an AI system's objectives are mis-specified or (b) informational asymmetries exist between human principals and artificial agents. On this description, misalignment arises along three interacting axes: objectives (when reward functions fail to capture true goals), information (when opacity or distributional shifts distort performance), and principals (reflecting plural, sometimes conflicting, human interests among developers, users, and affected stakeholders). Viewing alignment through these axes situates it within broader multi-agent coordination problems, revealing that alignment is not a unitary problem with a final solution but an ongoing matter of mitigation. This structural account integrates socio-technical factors—such as power, labour, and environmental dynamics—thereby providing conceptual clarity and practical guidance for addressing misalignment in both current and future AI systems.
 <br>
      </p>
      <p>
         <a href="" title="Relative Principals, Draft">[Preprint forthcoming.]</a> (Please cite official version, if available.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2025 "Relative Principals, Pluralistic Alignment, and the Structural Value Alignment Problem." <i>Unpublished Manuscript</i>. October 2025, PDF File.<br>
      </p>
   </div>
</details>

<br>

<details>
   <summary><a> <strong> What Russell Can Denote: Aboutness and Denotation Between <i>Principles</i> and 'On Denoting' </strong></a> </summary>
   <div class="indented">
      <p>
         How ought we to analyse propositions that are about nonexistent entities? Russell (1903) details the concept of <i>denoting</i> in <i>Principles of Mathematics</i>, and this theory appears to answer the question posed. However, in the paper 'On Denoting' (Russell 1905), we see that his theory of denoting has changed greatly. Hylton (1990) argues that the move from the former theory to the latter was unnecessary. The purpose of this paper is to show that, contra Hylton, the move to the theory found in 'On Denoting' was indeed necessary.</p>
         <p>I argue that Hylton is correct to the extent that an answer to our first question relies on a different question concerning the ontology of nonexistent entities. However, this fails to take into account is a more interesting question regarding the truth values of propositions containing such puzzling entities. This question relies on Russell's notion of aboutness, and in this sense is more sensitive to his theory as a complete picture of denotation. If we take the aboutness relation seriously, then we see that the move from the former theory to the latter was necessary after all.<br>
      </p>
      <p>
         <a href="https://travislacroix.github.io/files/ND-What-Russell-Can-Denote.pdf" title="What Russell Can Denote, Draft">[Unpublished Draft Available Here.]</a> (Please cite official version, if available.) <br>
      </p>
      <p>
         <b>Recommended citation</b>: <br> LaCroix, Travis. 2019. "What Russell Can Denote: Aboutness and Denotation Between <i>Principles</i> and 'On Denoting'." <i>Unpublished Manuscript</i>. May 2019, PDF File.<br>
      </p>
   </div>
</details>

<h2>Selected Working Papers</h2>

<details>
   <summary><a> <strong> Reference by Proxy and Truth-in-a-Model </strong> </a> </summary>
   <div class="indented">
      <p>
         Simchen (2017) brings to light the notion of 'scrambled truth' to show how productivist metasemantics is able to deal with problems of singular reference in a way that an interpretationist metasemantics (such as Lewisian reference magnetism) cannot. This serves to show that productivism is a live alternative, and indeed a rival to interpretationist metasemantics, and so cannot be subsumed by interpretationist theories.
      </p>
      <p>
         I examine Simchen's challenge to interpretationist metasemantics by extending his theoretical problem in light of actual communicative exchanges. I show that when the problem is couched in these terms, the ability to refer depends inherently upon coordination—the onus of which is on the receiver. Thus, I show how the interpretationist stance, in this case, can reasonably be understood to encompass the productivist stance.
      </p>
   </div>
</details>

<br>

<details>
   <summary><a> <strong> Saltationist versus Gradualist Approaches to Language Origins: A Critical Discussion </strong> </a> </summary>
   <div class="indented">
      <p>
         In spite of their vast differences, theories of language origins can be, more or less, partitioned into two exhaustive and mutually exclusive camps: <i>saltationist</i> and <i>gradualist</i>. Saltationism—from the Latin <i>saltus</i>, meaning 'leap'—is the view that (the human-level capacity for) language sprang into existence suddenly and recently, and that there is a complete discontinuity between the linguistic capacities of humans and the communication systems of non-human animals; whereas, gradualism—from the Latin <i>gradus</i>, meaning 'step'—is the view that language evolved slowly over long periods of time. However, rather than arguing for the plausibility of a gradualist versus a saltationist scenario, most researchers appear to fall into one or the other camp based purely upon external or pre-theoretic commitments regarding what they believe evolved [emerged] and how.
      </p>
      <p>
         The purpose of this paper is to critically survey the respective commitments and entailments of saltationist and gradualist theories of language origins in order to make an explicit argument that the saltationist view is theoretically untenable. Under scrutiny, holding one or the other theoretical stance toward language origins will require or entail certain commitments, which vary in plausibility. It appears that many researchers either ignore these facts or are willing to bite the bullet with respect to them. However, arguments for why one should be so willing are often few and far between.
      </p>
   </div>
</details>

<br>


